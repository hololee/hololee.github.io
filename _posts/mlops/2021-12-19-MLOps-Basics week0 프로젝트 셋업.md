---
title: MLOps-Basics week0 í”„ë¡œì íŠ¸ ì…‹ì—…
date: 2021-12-19 21:00:00 +0900
categories: [ mlops]
tags: [ai, mlops, devops]     # TAG names should always be lowercase
img_path: /
---

>ì´ í¬ìŠ¤íŠ¸ëŠ” ì‘ì„±ìì˜ í—ˆë½ì„ ë°›ì•„ ë²ˆì—­í•˜ê³  ê²Œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤. ë‚´ìš©ê³¼ ê´€ë ¨í•œ ì§ˆë¬¸ì€ [ì‘ì„±ìì˜ repo](https://github.com/graviraja/MLOps-Basics)ë¥¼ ì´ìš©í•´ì£¼ì„¸ìš”.
{: .prompt-info}

**Note: ì´ í”„ë¡œì íŠ¸ì˜ ëª©ì ì€ ëª¨ë¸ì˜ SOTA ë‹¬ì„±ì´ ì•„ë‹Œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ì•Œì•„ë³´ê³  ë°°ìš°ëŠ”ë° ëª©ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. **

# Requirements:

ì´ í”„ë¡œì íŠ¸ì—ì„œëŠ” Python 3.8 ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ë‹¤ìŒ ì»¤ë©˜ë“œë¥¼ ì´ìš©í•´ì„œ ê°€ìƒ í™˜ê²½ì„ ìƒì„±í•©ë‹ˆë‹¤:

```bash
conda create --name project-setup python=3.8
conda activate project-setup
```

requirementsë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤:

```bash
pip install -r requirements.txt
```

# ì‹¤í–‰

### í•™ìŠµ

requirementsë¥¼ ì„¤ì¹˜í•œ í›„ì— ëª¨ë¸ì„ í•™ìŠµì„ ê°„ë‹¨í•˜ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
python train.py
```

### ì¶”ë¡ 

í•™ìŠµì´ ëë‚œ í›„ ëª¨ë¸ checkpoint pathë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ì•„ë˜ì²˜ëŸ¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

```bash
python inference.py
```

### notebooks ì‹¤í–‰í•˜ê¸°

ì‘ê°€ëŠ” notebookì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„œ  [Jupyter lab](https://jupyter.org/install)ì„ ì´ìš©í•©ë‹ˆë‹¤. 

virtualenvë¥¼ ì‚¬ìš©í•˜ê¸°ë•Œë¬¸ì—,  `jupyter lab` ì»¤ë©˜ë“œë¥¼ ì´ìš©í• ë•Œ virtualenvë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ì•ˆí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

virutalenvë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œ `jupyter lab`ì‹¤í–‰ì „ ì•„ë˜ì˜ ì»¤ë§¨ë“œë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.

```bash
conda install ipykernel
python -m ipykernel install --user --name project-setup
pip install ipywidgets
```
<br/>
<br/>
<br/>

# MLOps ë² ì´ì§ [Week 0]: Project ì„¤ì •
<br/>

## ğŸ¬ ê°•ì˜ ì‹œì‘    
ì´ ê°•ì˜ì˜ ëª©í‘œëŠ” MLOpsì˜ ê¸°ë³¸ì ì¸ ìš”ì†Œë“¤(eg. ëª¨ë¸ ë¹Œë“œ, ëª¨ë‹ˆí„°ë§, ì„¤ì •, í…ŒìŠ¤íŠ¸, í˜í‚¤ì§•, ë°°í¬, CI/CD)ì„ ì´í•´í•˜ëŠ” ê²ƒ ì…ë‹ˆë‹¤. ì²«ë²ˆì§¸ë¡œ í”„ë¡œì íŠ¸ë¥¼ ì„¤ì •í•´ë´…ì‹œë‹¤. ì €ìëŠ” NLPì— ê´€ì‹¬ì´ ë§ì•„ì„œ NLPëª¨ë¸ ìœ„ì£¼ë¡œ ì„¤ëª…ì´ ì§„í–‰ë˜ì§€ë§Œ ê¸°ë³¸ì ì¸ ì ˆì°¨ëŠ” ëª¨ë‘ ë¹„ìŠ·í•˜ê²Œ ì§„í–‰ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ classification taskë¥¼ ê°€ì§€ê³  ì§„í–‰í•©ë‹ˆë‹¤.

ì´ ê¸€ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì€ ì§ˆë¬¸ë“¤ì„ ë‹¤ë¤„ë´…ë‹ˆë‹¤.

- <b>`ë°ì´í„°ë¥¼ ì–»ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€?`</b>
- <b>`ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í•  ê²ƒì¸ê°€?`</b>
- <b>`dataloaderë¥¼ ì •ì˜í•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€?`</b>
- <b>`ëª¨ë¸ì€ ì–´ë–»ê²Œ ì„ ì–¸í•˜ëŠ” ê²ƒì¸ê°€?`</b>
- <b>`ëª¨ë¸ì„ ì–´ë–»ê²Œ í•™ìŠµí•˜ëŠ”ê°€?`</b>
- <b>`ëª¨ë¸ ì¶”ë¡ ì„ ì–´ë–»ê²Œ í•˜ëŠ” ê²ƒì¸ê°€?`</b>

<i>ë…¸íŠ¸: ê¸°ë³¸ì ì¸ ë¨¸ì‹ ëŸ¬ë‹ì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•©ë‹ˆë‹¤.</i>   
<br/>
<br/>

## ğŸ›  ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬   
ë”¥ëŸ¬ë‹ í”„ë¡œì íŠ¸ë¥¼ ê°œë°œí•˜ê¸° ìœ„í•´ì„œ ì•„ë˜ì™€ ê°™ì€ ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- <b>[Tensorflow](https://www.tensorflow.org/)</b>
- <b>[Tensorflow Lite](https://www.tensorflow.org/lite)</b>
- <b>[Pytorch](https://pytorch.org/)</b>
- <b>[Pytorch Lightning](https://www.pytorchlightning.ai/)</b>
- etc..

ì €ìëŠ” ì—¬ëŸ¬ íŠ¹ì„±ê³¼ ìë™í™”ëœ ì½”ë“œë¥¼ í™œìš©í•˜ê¸° ìœ„í•´ì„œ `Pytorch Lightning`ì„ ì´ìš©í•©ë‹ˆë‹¤.
<br/>
<br/>

## ğŸ“š ë°ì´í„° ì…‹   
ì—¬ê¸°ì„œëŠ” `CoLA`(Corpus of Linguistic Acceptability) ë°ì´í„° ì…‹ì„ ì´ìš©í•©ë‹ˆë‹¤. ì£¼ì–´ì§€ëŠ” ë¬¸ì¥ì´ ë¬¸ë²•ì ìœ¼ë¡œ ë§ëŠ”ì§€, ì•„ë‹Œì§€ 2ê°œì˜ classë¡œ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì„ ì§„í–‰í•´ë´…ì‹œë‹¤.

- âŒ `Unacceptable`: ë¬¸ë²•ì ìœ¼ë¡œ ë§ì§€ ì•ŠìŒ
- âœ… `Acceptable`: ë¬¸ë²•ì ìœ¼ë¡œ ë§ìŒ

ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œ í•˜ê³  ë¡œë“œí•˜ê¸° ìœ„í•´ì„œ [Huggingface datasets](https://huggingface.co/docs/datasets/quicktour.html)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” 800+ì˜ ë°ì´í„°ì…‹ì„ ì§€ì›í•˜ê³  ì»¤ìŠ¤í…€ ë°ì´í„°ë„ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   

ì•„ë˜ì™€ ê°™ì´ ì‰½ê²Œ ë‹¤ìš´ë¡œë“œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
~~~python
cola_dataset = load_dataset("glue", "cola")
print(cola_dataset)
~~~
~~~python
DatasetDict({
    train: Dataset({
        features: ['sentence', 'label', 'idx'],
        num_rows: 8551
    })
    validation: Dataset({
        features: ['sentence', 'label', 'idx'],
        num_rows: 1043
    })
    test: Dataset({
        features: ['sentence', 'label', 'idx'],
        num_rows: 1063
    })
})
~~~

ë°ì´í„°ë¥¼ í•œë²ˆ ì‚´í´ë´…ë‹ˆë‹¤.
~~~python
train_dataset = cola_dataset['train']
print(train_dataset[0])
~~~
~~~json
{
    'idx': 0,
    'label': 1,
    'sentence': "Our friends won't buy this analysis, let alone the next one we propose."
}
~~~
<br/>
<br/>

## ğŸ›’ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°   
ë°ì´í„° íŒŒì´í”„ë¼ì¸ì€ ì•„ë˜ì˜ ë„êµ¬ë¥¼ ì´ìš©í•´ì„œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   

- ğŸ¦ Vanilla Pytorch [DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)
- âš¡ Pytorch Lightning [DataModules](https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html)   
  

`DataModules`ê°€ CPU & GPUì— ë” ìµœì í™” ë˜ì–´ìˆê³  êµ¬ì¡°í™”ê°€ ì˜ ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê°€ëŠ¥í•˜ë‹¤ë©´ `DataModules`ì„ ì´ìš©í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤.   

`DataModules`ì€ ë‹¤ìŒê³¼ ê°™ì€ interfaceì— ì˜í•´ ì •ì˜ë©ë‹ˆë‹¤.   
- `prepare_data` (optional) ëŠ” 1ê°œì˜ GPUì—ì„œ í•œë²ˆë§Œ í˜¸ì¶œë©ë‹ˆë‹¤. -- ì¼ë°˜ì ìœ¼ë¡œ ì•„ë˜ì—ì„œ ì„¤ëª…í•  ë°ì´í„° ë‹¤ìš´ë¡œë“œí•˜ëŠ” ê²ƒê³¼ ê°™ì€ ì‘ì—…ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   
- `setup`ì€ ê°ê° GPUì—ì„œ í˜¸ì¶œë˜ë©° **fit**ë˜ëŠ” **test**ë‹¨ê³„ì— ìˆëŠ”ì§€ ì •ì˜í•˜ê¸° ìœ„í•´ì„œ **stage**ë¥¼ ì´ìš©í•©ë‹ˆë‹¤.
- ê° ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ì„œ `train_dataloader`, `val_dataloader` and `test_dataloader`ë¥¼ ì´ìš©í•©ë‹ˆë‹¤.

`DataModule`ì€ PyTorchì—ì„œ ë°ì´í„° ì²˜ë¦¬ì— ê´€ì—¬í•˜ëŠ” 5ê°€ì§€ ìŠ¤í…ìœ¼ë¡œ ìº¡ìŠí™” ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- ì „ì²˜ë¦¬ ë‹¨ê³„(ë‹¤ìš´ë¡œë“œ/ í† í°í™”/ process)
- ì •ë¦¬í•˜ê±°ë‚˜ ë””ìŠ¤í¬ì— ì €ì¥í•˜ê¸°
- Datasetìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
- transforms ì ìš©(íšŒì „, í† í°í™”, ê¸°íƒ€â€¦)
- DataLoaderë¡œ ê°ì‹¸ê¸°

í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” `DataModule`ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
~~~python
class DataModule(pl.LightningDataModule):
    def __init__(self, model_name="google/bert_uncased_L-2_H-128_A-2", batch_size=32):
        super().__init__()

        self.batch_size = batch_size
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

    def prepare_data(self):
        cola_dataset = load_dataset("glue", "cola")
        self.train_data = cola_dataset["train"]
        self.val_data = cola_dataset["validation"]

    def tokenize_data(self, example):
        # processing the data
        return self.tokenizer(
            example["sentence"],
            truncation=True,
            padding="max_length",
            max_length=256,
        )

    def setup(self, stage=None):
        if stage == "fit" or stage is None:
            self.train_data = self.train_data.map(self.tokenize_data, batched=True)
            self.train_data.set_format(
                type="torch", columns=["input_ids", "attention_mask", "label"]
            )

            self.val_data = self.val_data.map(self.tokenize_data, batched=True)
            self.val_data.set_format(
                type="torch", columns=["input_ids", "attention_mask", "label"]
            )

    def train_dataloader(self):
        return torch.utils.data.DataLoader(
            self.train_data, batch_size=self.batch_size, shuffle=True
        )

    def val_dataloader(self):
        return torch.utils.data.DataLoader(
            self.val_data, batch_size=self.batch_size, shuffle=False
        )
~~~
<br/>
<br/>

## ğŸ—ï¸ Lightningì„ ì´ìš©í•œ ëª¨ë¸ êµ¬ì„±    
PyTorch Lightningì—ì„œ ëª¨ë¸ì€ [LightningModule](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html)ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ `torch.nn.Module`(ğŸ¦)ìœ„ì— ë‹¤ì–‘í•œ functionë“¤(ğŸ¨)ì´ ì˜¬ë¼ê°€ ìˆëŠ” êµ¬ì¡° ì…ë‹ˆë‹¤(ë°”ë‹ë¼ ì•„ì´ìŠ¤í¬ë¦¼ ìœ„ì— ì²´ë¦¬!).
ì´ ì²´ë¦¬ë“¤ì€ ë°˜ë³µë˜ëŠ” ì½”ë“œë¥¼ ì¤„ì¼ ìˆ˜ ìˆê³  ì—”ì§€ë‹ˆì–´ë§ ì½”ë“œë¥¼ ë¨¸ì‹ ëŸ¬ë‹ ì½”ë“œì™€ ë¶„ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.

ì˜ˆë¥¼ë“¤ë©´, í•œ epochì— ë°˜ë³µë˜ëŠ” batchì—ì„œ ì–´ë– í•œ ë™ì‘ì´ ì´ë£¨ì–´ì§€ëŠ”ì§€ë¥¼ `training_step`ì„ ì´ìš©í•´ì„œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   

ëª¨ë¸ì„ `LightningModule`ë¡œ ë™ì‘í•˜ê²Œ í•˜ë ¤ë©´ ìƒˆë¡œìš´ `class`ë¥¼ ì •ì˜í•˜ê³  ëª‡ê°€ì§€ ë©”ì„œë“œë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.   

`LightningModule`ì€ ë‹¤ìŒê³¼ ê°™ì€ interfaceì— ì˜í•´ ì •ì˜ë©ë‹ˆë‹¤.
- `init` ëª¨ë“ˆì˜ ì´ˆê¸° ì„¤ì •ì„ ì •ì˜í•©ë‹ˆë‹¤.
- `forward` ì£¼ì–´ì§€ëŠ” ì…ë ¥ì— ëŒ€í•´ì„œ ì–´ë– í•œ ë™ì‘ì„ í• ì§€ ì •ì˜í•©ë‹ˆë‹¤(loss ê³„ì‚°, weight ì—…ë°ì´íŠ¸ëŠ” ì œì™¸í•©ë‹ˆë‹¤.).
- `training_step` training stepìœ¼ë¡œ loss ê³„ì‚°ì´ë‚˜ metric ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. weightì—…ë°ì´íŠ¸ëŠ” í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- `validation_step` validation step
- `test_step` test step (optional)
- `configure_optimizers` ì–´ë–¤ optimizerë¥¼ ì´ìš©í• ì§€ ì„¤ì •í•©ë‹ˆë‹¤.   

ì´ì™¸ì—ë„ ë‹¤ì–‘í•œ ê¸°ëŠ¥ë“¤ì„ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ì •ë³´ëŠ” [ì—¬ê¸°](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html)ë¥¼ í™•ì¸í•˜ì„¸ìš”.   

ì´ í”„ë¡œì íŠ¸ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ `LightningModule`ì„ ì´ìš©í•©ë‹ˆë‹¤.
~~~python
class ColaModel(pl.LightningModule):
    def __init__(self, model_name="google/bert_uncased_L-2_H-128_A-2", lr=1e-2):
        super(ColaModel, self).__init__()
        self.save_hyperparameters()

        self.bert = AutoModel.from_pretrained(model_name)
        self.W = nn.Linear(self.bert.config.hidden_size, 2)
        self.num_classes = 2

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)

        h_cls = outputs.last_hidden_state[:, 0]
        logits = self.W(h_cls)
        return logits

    def training_step(self, batch, batch_idx):
        logits = self.forward(batch["input_ids"], batch["attention_mask"])
        loss = F.cross_entropy(logits, batch["label"])
        self.log("train_loss", loss, prog_bar=True)
        return loss

    def validation_step(self, batch, batch_idx):
        logits = self.forward(batch["input_ids"], batch["attention_mask"])
        loss = F.cross_entropy(logits, batch["label"])
        _, preds = torch.max(logits, dim=1)
        val_acc = accuracy_score(preds.cpu(), batch["label"].cpu())
        val_acc = torch.tensor(val_acc)
        self.log("val_loss", loss, prog_bar=True)
        self.log("val_acc", val_acc, prog_bar=True)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.hparams["lr"])
~~~
<br/>
<br/>

## ğŸ‘Ÿ í•™ìŠµ   
`DataLoader`, `LightningModule`ëŠ” `Trainer`ì˜í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. `Trainer`ëŠ” ë°ì´í„° ë¡œë“œ, gradient ê³„ì‚°, optimizer ë¡œì§, ë¡œê¹…ë“±ì„ ì¡°ìœ¨í•´ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.   

`Trainer`ëŠ” ì—¬ëŸ¬ê°€ì§€ ì˜µì…˜ë“¤ë¡œ ë¡œê¹…, ê·¸ë¼ë””ì–¸íŠ¸ ì¶•ì , half precision training, ë¶„ì‚° ì»´í“¨íŒ… ë“±ê³¼ ê°™ì´ ì»¤ìŠ¤í…€ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   

ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ì ì¸ ì˜ˆì œë¥¼ ì´ìš©í•˜ê² ìŠµë‹ˆë‹¤.
~~~python
cola_data = DataModule()
cola_model = ColaModel()

trainer = pl.Trainer(
    gpus=(1 if torch.cuda.is_available() else 0),
    max_epochs=1,
    fast_dev_run=False,
)
trainer.fit(cola_model, cola_data)
~~~

`fast_dev_run=True`ë¡œ ì„¤ì •í•˜ë©´ training 1ìŠ¤í…,validation 1stepìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤(Trueë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ëŒ€ë¶€ë¶„ ì¢‹ìŠµë‹ˆë‹¤. validation ìŠ¤í…ì—ì„œ ì˜ëª»ëœ ë¶€ë¶„ì´ ë°”ë¡œ ë°œìƒí•˜ê¸° ë–„ë¬¸ì— í•™ìŠµì´ ì™„ë£Œë ë•Œê¹Œì§€ ê¸°ë‹¤ë¦´ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.).   
<br/>
<br/>

## ğŸ“ ë¡œê¹…   
ëª¨ë¸ í•™ìŠµì„ ë¡œê¹…í•˜ëŠ” ê²ƒì€ ë‹¤ìŒê³¼ ê°™ì´ ê°„ë‹¨í•©ë‹ˆë‹¤.
~~~python
cola_data = DataModule()
cola_model = ColaModel()

trainer = pl.Trainer(
    default_root_dir="logs",
    gpus=(1 if torch.cuda.is_available() else 0),
    max_epochs=1,
    fast_dev_run=False,
    logger=pl.loggers.TensorBoardLogger("logs/", name="cola", version=1),
)
trainer.fit(cola_model, cola_data)
~~~

`logs/cola`ë””ë ‰í„°ë¦¬ê°€ ìƒì„±ë˜ê³  ì•„ë˜ì˜ ì»¤ë§¨ë“œë¥¼ ì´ìš©í•´ì„œ tensorboardë¡œ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
~~~bash
tensorboard --logdir logs/cola
~~~
í…ì„œë³´ë“œëŠ” `http://localhost:6006/`ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
<br/>
<br/>

## ğŸ” Callback   
`Callback`ì€ í”„ë¡œì íŠ¸ì— ì „ë°˜ì ìœ¼ë¡œ ì¬í™œìš©ë  ìˆ˜ ìˆëŠ” ë‚´ì¥ëœ í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤.

ì˜ˆë¥¼ë“¤ì–´, **ModelCheckpoint** callbackì„êµ¬í˜„í•œë‹¤ê³  í•©ë‹ˆë‹¤. ì´ callbackì€ í•™ìŠµëœ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.   
metricì„ ëª¨ë‹ˆí„°ë§í•´ì„œ ì–´ë–¤ ëª¨ë¸ì„ ì €ì¥í• ì§€ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì—¬ê¸°ì„œëŠ” `val_loss`ë¥¼ ì´ìš©í•©ë‹ˆë‹¤.). ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì€ `dirpath`ì— ì €ì¥ë©ë‹ˆë‹¤.   

Callbackì— ëŒ€í•œ ìì„¸í•œ ì •ë³´ëŠ” [ì—¬ê¸°](https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html)ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.   
~~~python
cola_data = DataModule()
cola_model = ColaModel()

checkpoint_callback = ModelCheckpoint(
    dirpath="./models", monitor="val_loss", mode="min"
)

trainer = pl.Trainer(
    default_root_dir="logs",
    gpus=(1 if torch.cuda.is_available() else 0),
    max_epochs=1,
    fast_dev_run=False,
    logger=pl.loggers.TensorBoardLogger("logs/", name="cola", version=1),
    callbacks=[checkpoint_callback],
)
trainer.fit(cola_model, cola_data)
~~~

ë˜í•œ callbackì„ ì—¬ëŸ¬ê°œ ì—®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. `EarlyStopping`callbackì€ íŠ¹ì • íŒŒë¼ë¯¸í„°(ì—¬ê¸°ì„œëŠ” `val_loss`ë¥¼ ì´ìš©í•©ë‹ˆë‹¤.)ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ë©´ì„œ ëª¨ë¸ì˜ overfitì„ ë°©ì§€í•©ë‹ˆë‹¤.   
~~~python
early_stopping_callback = EarlyStopping(
    monitor="val_loss", patience=3, verbose=True, mode="min"
)

trainer = pl.Trainer(
    default_root_dir="logs",
    gpus=(1 if torch.cuda.is_available() else 0),
    max_epochs=1,
    fast_dev_run=False,
    logger=pl.loggers.TensorBoardLogger("logs/", name="cola", version=1),
    callbacks=[checkpoint_callback, early_stopping_callback],
)
trainer.fit(cola_model, cola_data)
~~~
<br/>
<br/>

## ğŸ” ì¶”ë¡    
ëª¨ë¸ì´ í•™ìŠµë˜ê³ ë‚˜ë©´ ì˜ˆì¸¡ì„ìœ„í•´ì„œ í•™ìŠµëœ ëª¨ë¸ì„ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   
ì¼ë°˜ì ìœ¼ë¡œ `Inference`ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.
- í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
- ëŸ°íƒ€ì„ ì…ë ¥ ì–»ê¸°
- ì…ë ¥ì„ ì•Œë§ì€ í¬ë©§ìœ¼ë¡œ ë³€ê²½í•˜ê¸°
- ì˜ˆì¸¡ê°’ ì–»ê¸°
~~~python
class ColaPredictor:
    def __init__(self, model_path):
        self.model_path = model_path
        # loading the trained model
        self.model = ColaModel.load_from_checkpoint(model_path)
        # keep the model in eval mode
        self.model.eval()
        self.model.freeze()
        self.processor = DataModule()
        self.softmax = torch.nn.Softmax(dim=0)
        self.lables = ["unacceptable", "acceptable"]

    def predict(self, text):
        # text => run time input
        inference_sample = {"sentence": text}
        # tokenizing the input
        processed = self.processor.tokenize_data(inference_sample)
        # predictions
        logits = self.model(
            torch.tensor([processed["input_ids"]]),
            torch.tensor([processed["attention_mask"]]),
        )
        scores = self.softmax(logits[0]).tolist()
        predictions = []
        for score, label in zip(scores, self.lables):
            predictions.append({"label": label, "score": score})
        return predictions
~~~

ì´ í¬ìŠ¤íŠ¸ì— ë‚˜ì™€ìˆëŠ” ì „ì²´ ì½”ë“œëŠ” ë‹¤ìŒ ë§í¬ì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤: [Github](https://github.com/graviraja/MLOps-Basics)